{
 "metadata": {
  "name": "",
  "signature": "sha256:470420df0e975343af30870350596853b249044513c96fba479e602bc32256f5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "18-889e: Home work 1 Solution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note for Venkat: If you can open this, you have successfully managed to install ipython and the ipython notebook. Basically this forms a neat way for me to view the progress of my ideas so I tend to use it for homeworks. We can switch to scripts if you prefer that instead as well. Also, I've imported the 'numpy' package into python. The syntax of using that is close to Matlab so I thought it would make things more comfortable for you. \n",
      "The whole program is divided into cells that must be run in the order they appear. Basically you have to keep hitting 'Shift+Enter' to run the whole program. You can also do that on this cell."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Solution to the first homework of Real life reinforcement learning\n",
      "import csv\n",
      "import numpy as np\n",
      "from sklearn import linear_model\n",
      "from sklearn import neighbors\n",
      "\n",
      "# First read in all the data from the file.\n",
      "with open('generated_episodes_3000.csv') as csv_file:\n",
      "    data = np.array(list(csv.reader(csv_file))[1:])\n",
      "\n",
      "# At this point all the data has been loaded into 'data'. I've ignored the first row because those are just labels that are \n",
      "# useless to us."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Uncomment the next statement if you want to see what a row of the data looks like.\n",
      "# data[0]\n",
      "# data[0].shape\n",
      "#print data[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import preprocessing\n",
      "def get_known_states(data):\n",
      "    \"\"\"\n",
      "        Returns just the states that we know, i.e. states without the 'NA' in the data fields.\n",
      "    \"\"\"\n",
      "    event_length = 9 + 9 + 2\n",
      "    state_length = 9 + 2\n",
      "    num_states = 24\n",
      "    for episode in data:\n",
      "        # Start at the beginning and keep looking at a net length of len(s) + len(a) + len(r) + len(s') points\n",
      "        # Each time, we increment our start position by s+a+r = 11 points\n",
      "        curr_state = 0\n",
      "        while curr_state < num_states:\n",
      "            start_idx = curr_state * state_length\n",
      "            end_idx = start_idx + event_length\n",
      "            datum = episode[start_idx:end_idx]\n",
      "            try:\n",
      "                s = datum[:9].astype(np.float)\n",
      "                yield s\n",
      "            # There's a problem if a data field is 'NA' - Not entirely sure what do in that case so for now I'm just ignoring\n",
      "            # those data points\n",
      "            except ValueError:\n",
      "                pass    \n",
      "            curr_state += 1\n",
      "            \n",
      "def generate_sars(data):\n",
      "    \"\"\"\n",
      "        Function that returns the next (s, a, r, s') pair from the input data one by one every time you call it. Requires a \n",
      "        scaler to have been computed so that we can approximate the vaues for the 'NA' pairs in the data.\n",
      "    \"\"\"\n",
      "    # Compute the known states and then compute a 'scaler' which stores the means and variances that will be used for standardization.\n",
      "    known_states = [state for state in get_known_states(data)]\n",
      "    scaler = preprocessing.StandardScaler().fit(known_states)\n",
      "    event_length = 9 + 9 + 2\n",
      "    state_length = 9 + 2\n",
      "    num_states = 24\n",
      "    sars = []\n",
      "    for episode in data:\n",
      "        # Start at the beginning and keep looking at a net length of len(s) + len(a) + len(r) + len(s') points\n",
      "        # Each time, we increment our start position by s+a+r = 11 points\n",
      "        curr_state = 0\n",
      "        while curr_state < num_states:\n",
      "            start_idx = curr_state * state_length\n",
      "            end_idx = start_idx + event_length\n",
      "            datum = episode[start_idx:end_idx]\n",
      "            # If its normal data without 'NA', proceed as before except we 'scale' the values to mean-0 and variance-1\n",
      "            a = 1.0 if datum[9:10]=='true' else 0.0\n",
      "            r = np.asscalar(datum[10:11].astype(np.float))\n",
      "            try:\n",
      "                s = datum[:9].astype(np.float)\n",
      "                scaler.transform(s)\n",
      "                s_prime = datum[11:].astype(np.float)       \n",
      "                scaler.transform(s_prime)\n",
      "                sars.append([s, a, r, s_prime])\n",
      "            # IF there was a value error it means there was a 'NA' field somewhere. \n",
      "            except ValueError:\n",
      "                # ONLY S AND S' have these 'NA' fields (I've confirmed). Therefore we go through them and replace any\n",
      "                # fields that have 'NA' with the mean of the corresponding feature, and then apply the scaler.\n",
      "                s = np.array([elem if elem!='NA' else scaler.mean_[i].astype(np.float) for i, elem in enumerate(datum[:9])]).astype(np.float)\n",
      "                scaler.transform(s)\n",
      "                s_prime = np.array([elem if elem!='NA' else scaler.mean_[i].astype(np.float) for i, elem in enumerate(datum[:9])]).astype(np.float)\n",
      "                scaler.transform(s_prime).astype(np.float)\n",
      "                sars.append([s, a, r, s_prime])\n",
      "            curr_state += 1\n",
      "    return sars"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def FVI(fn, sars, gamma = 0.999, iters = 1):\n",
      "    \"\"\"\n",
      "        Does fixed value iteration using the input fn approximator on the input data (expected to be in SARS format)\n",
      "    \"\"\"\n",
      "    # Initialize the weights. Do one iteration to get things started\n",
      "    Xs = []\n",
      "    ys = []\n",
      "    for s, a, r, s_prime in sars:\n",
      "        y = r\n",
      "        Xs.append(np.append(s, a))\n",
      "        ys.append(y)\n",
      "    fn.fit(Xs, ys)\n",
      "    for i in range(iters):\n",
      "        Xs = []\n",
      "        ys = []\n",
      "        for s, a, r, s_prime in sars:\n",
      "            y = r + gamma * max(fn.predict(np.append(s_prime, 0.0)), fn.predict(np.append(s_prime, 1.0)))\n",
      "            Xs.append(np.append(s, a))\n",
      "            ys.append(y[0])\n",
      "        fn.fit(Xs, ys)\n",
      "    return fn"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 107
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "With that the main loop of our FVI code is pretty simply defined as below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# A key difference here is that we're just precomputing and storing all the s, a, r, s_prime pairs ahead of time. No generator\n",
      "# functions here.\n",
      "sars = generate_sars(data)\n",
      "\n",
      "# And now we'll have to do the following in a loop - Currently this is one iteration of proper FVI\n",
      "fn = linear_model.Lasso(alpha = 0.1)\n",
      "\n",
      "# Uncomment for K-NN\n",
      "#n_neighbors = 5\n",
      "#fn = neighbors.KNeighborsRegressor(n_neighbors, weights=\"distance\")\n",
      "\n",
      "FVI(fn, sars)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 108,
       "text": [
        "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=1000,\n",
        "   normalize=False, positive=False, precompute=False, random_state=None,\n",
        "   selection='cyclic', tol=0.0001, warm_start=False)"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO:\n",
      "\n",
      "-> Figure out- How many iterations do we have to do that last cell for? When do we say we've converged\n",
      "\n",
      "-> What to do about the 'NA' data - Currently ignoring it but thats a very temporary solution\n",
      "\n",
      "-> Try other function approximators"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}